{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  result|\n",
      "+--------+\n",
      "| aibgraz|\n",
      "| default|\n",
      "|    test|\n",
      "|   test2|\n",
      "|weizmann|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('use weizmann')\n",
    "sqlContext.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dataset='image_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] table exists\n"
     ]
    }
   ],
   "source": [
    "path_prefix_ground='/home/sh/Data/TestData/WeizMaan_horse/weizmann_horse_db/figure_ground'\n",
    "path_prefix_gray ='/home/sh/Data/TestData/WeizMaan_horse/weizmann_horse_db/gray'\n",
    "path_prefix_rgb ='/home/sh/Data/TestData/WeizMaan_horse/weizmann_horse_db/rgb'\n",
    "\n",
    "if update==True:\n",
    "    sqlContext.sql('drop table if exists %s'%(image_dataset))\n",
    "    print('[info] info mode, drop table if exists')\n",
    "\n",
    "if image_dataset not in sqlContext.sql('show tables').rdd.map(lambda row : row.tableName).collect():\n",
    "    import os\n",
    "    import os.path\n",
    "    list_dataset =[]\n",
    "    list_file = os.listdir(path_prefix_rgb)\n",
    "    id = 1\n",
    "    for file_name in list_file:\n",
    "        path_rgb = os.path.join(path_prefix_rgb, file_name)\n",
    "        path_gray = os.path.join(path_prefix_gray, file_name)\n",
    "        path_ground = os.path.join(path_prefix_ground, file_name)\n",
    "    \n",
    "        list_dataset.append(Row(id=id, rgb=path_rgb, gray=path_gray, ground = path_ground))\n",
    "        id = id + 1  \n",
    "    \n",
    "    rdd_dataset = sc.parallelize(range(0, len(list_dataset))).map(lambda i: list_dataset[i])\n",
    "    df_dataset = sqlContext.createDataFrame(rdd_dataset)\n",
    "    df_dataset.write.saveAsTable(name=image_dataset)\n",
    "    print('[info] Done')\n",
    "else:\n",
    "    print('[info] table exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|    tableName|isTemporary|\n",
      "+-------------+-----------+\n",
      "|image_dataset|      false|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_split = 'image_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(row):\n",
    "    if row.id <=200:\n",
    "        split='train'\n",
    "    else:\n",
    "        split='test'\n",
    "    return Row(id=row.id, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_dataset=sqlContext.sql('SELECT id FROM %s'%(image_dataset)).rdd\n",
    "rdd_split=rdd_dataset.map(split)\n",
    "df_split=sqlContext.createDataFrame(rdd_split)\n",
    "df_split.write.saveAsTable(name=image_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mathmatics background\n",
    "\n",
    "## CRF-based segmentation\n",
    "Given an Image $\\mathbf{x}$ and its corresponding labelling $\\mathbf{y}$, CRF(conditional random filed) models the conditional distribution of the form\n",
    "$$\n",
    "P(\\mathbf{y} | \\mathbf{x}; \\mathbf{W})=\\frac{1}{Z}exp(-E(\\mathbf{y,x;W}))\n",
    "$$\n",
    "where $\\mathbf{w}$ are paremeters and Z is the normalization term. The energy E of an image $\\mathbf{x}$ with segmentation labels $\\mathbf{y}$ over the the nodes $\\mathcal{N}$ and edges $\\mathcal{S}$ takes the following form:\n",
    "$$\n",
    "E(\\mathbf{y,x;w})=\\sum_{p\\in\\mathcal{N}}{\\Phi^{(1)}(y^p, \\mathbf{x}; \\mathbf{w})} + \\sum_{(p,q)\\in\\mathcal{S}}\\Phi^{(2)}(y^p, y^q, \\mathbf{x}; \\mathbf{w}).\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x}\\in\\mathcal{X}$; $\\mathbf{y}\\in\\mathcal{y}$ and $\\Phi^{(1)}$,$\\Phi^{(2)}$ are the unary and pairwise potentials.\n",
    "There are two setps in CRF-based segment, (1) learning the model parameters($w$) from the trainning data; (2) inferring a most likely label for the test data given the learned parameters. The segmentation problem thus reduced to find label $\\mathbf{y}$ with which get minimum energy, thus :\n",
    "$$\n",
    "\\mathbf{y^*} = \\arg\\min_{\\mathbf{y} \\in \\mathcal{y} }{ E(\\mathbf{y, x; w})}.\n",
    "$$\n",
    "\n",
    "## parameter of CRF\n",
    "To learn CRF with large margin framework, we make energy linear in the parameter $\\mathbf{w}$, so we define unary and pairwise potentials as:\n",
    "$$\n",
    "\\Phi^{(1)}(y^p, \\mathbf{x}; \\mathbf{w})=<\\mathbf{w}^{(1)},  \\phi^{(1)}(y^p, \\mathbf{x})>\n",
    "\\\\\n",
    "\\Phi^{(2)}(y^p, y^q, \\mathbf{x}; \\mathbf{w}) = <\\mathbf{w}^{(2)},  \\phi^{(2)}(y^p, y^q, \\mathbf{x})>\n",
    "$$\n",
    "where $\\phi^{(1)}$ and $\\phi^{(2)}$ are the unary and pairwise feature of superpixel $\\mathbf{x}$. So we get:\n",
    "$$\n",
    "\\mathbf{w}=[\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}]\n",
    "$$\n",
    "where $[\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)}]$ means $\\mathbf{w}^{(1)}$ concatenate $\\mathbf{w}^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# learning the model parameters\n",
    "\n",
    "## construct CRF from superpixel\n",
    "Superpixel become standard preprocess of \n",
    "## maximum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "sql_cmd='SELECT %s.id, %s.rgb, %s.ground \\\n",
    "         FROM %s join %s \\\n",
    "         ON %s.id = %s.id \\\n",
    "         WHERE %s.split==\"train\" '\\\n",
    "         %(image_dataset, image_dataset,image_dataset,\\\n",
    "           image_split, image_dataset,\\\n",
    "           image_split, image_dataset,\\\n",
    "           image_split)\n",
    "\n",
    "rdd_image_train=sqlContext.sql(sql_cmd).rdd\n",
    "print(rdd_image_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
